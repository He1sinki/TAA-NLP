{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c23afa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:18:24.424059Z",
     "start_time": "2024-01-12T07:18:22.959968Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=10000, suppress=True)\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f794df87208240d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Importer ce jeu de données avec la librairie pandas (c.f. read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc6a0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:18:25.878316Z",
     "start_time": "2024-01-12T07:18:24.426120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  Expression of p53 and coexistence of HPV in pr...   \n1  Vitamin D status in pregnant Indian women acro...   \n2  [Identification of a functionally important di...   \n\n                                        abstractText  \\\n0  Fifty-four paraffin embedded tissue sections f...   \n1  The present cross-sectional study was conducte...   \n2  The occurrence of individual amino acids and d...   \n\n                                           meshMajor      pmid  \\\n0  ['DNA Probes, HPV', 'DNA, Viral', 'Female', 'H...   8549602   \n1  ['Adult', 'Alkaline Phosphatase', 'Breast Feed...  21736816   \n2  ['Amino Acid Sequence', 'Analgesics, Opioid', ...  19060934   \n\n                                              meshid  \\\n0  [['D13.444.600.223.555', 'D27.505.259.750.600....   \n1  [['M01.060.116'], ['D08.811.277.352.650.035'],...   \n2  [['G02.111.570.060', 'L01.453.245.667.060'], [...   \n\n                                            meshroot  A  B  C  D  E  F  G  H  \\\n0  ['Chemicals and Drugs [D]', 'Organisms [B]', '...  0  1  1  1  1  0  0  1   \n1  ['Named Groups [M]', 'Chemicals and Drugs [D]'...  0  1  1  1  1  1  1  0   \n2  ['Phenomena and Processes [G]', 'Information S...  1  1  0  1  1  0  1  0   \n\n   I  J  L  M  N  Z  \n0  0  0  0  0  0  0  \n1  1  1  0  1  1  1  \n2  0  0  1  0  0  0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>abstractText</th>\n      <th>meshMajor</th>\n      <th>pmid</th>\n      <th>meshid</th>\n      <th>meshroot</th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n      <th>E</th>\n      <th>F</th>\n      <th>G</th>\n      <th>H</th>\n      <th>I</th>\n      <th>J</th>\n      <th>L</th>\n      <th>M</th>\n      <th>N</th>\n      <th>Z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Expression of p53 and coexistence of HPV in pr...</td>\n      <td>Fifty-four paraffin embedded tissue sections f...</td>\n      <td>['DNA Probes, HPV', 'DNA, Viral', 'Female', 'H...</td>\n      <td>8549602</td>\n      <td>[['D13.444.600.223.555', 'D27.505.259.750.600....</td>\n      <td>['Chemicals and Drugs [D]', 'Organisms [B]', '...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Vitamin D status in pregnant Indian women acro...</td>\n      <td>The present cross-sectional study was conducte...</td>\n      <td>['Adult', 'Alkaline Phosphatase', 'Breast Feed...</td>\n      <td>21736816</td>\n      <td>[['M01.060.116'], ['D08.811.277.352.650.035'],...</td>\n      <td>['Named Groups [M]', 'Chemicals and Drugs [D]'...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Identification of a functionally important di...</td>\n      <td>The occurrence of individual amino acids and d...</td>\n      <td>['Amino Acid Sequence', 'Analgesics, Opioid', ...</td>\n      <td>19060934</td>\n      <td>[['G02.111.570.060', 'L01.453.245.667.060'], [...</td>\n      <td>['Phenomena and Processes [G]', 'Information S...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('PubMed-multi-label-dataset.csv', sep=',', encoding=\"utf-8\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506c3d1cf628bd57",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:18:25.887437Z",
     "start_time": "2024-01-12T07:18:25.881132Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0572bd8f59d49a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Analyser votre jeu de données, essentiellement la target. Chaque texte est labélisée par un\n",
    "ensemble de labels parmi les 14 labels suivants : Anatomy [A], Organisms [B], Diseases [C], Chemicals and Drugs [D], Analytical, Diagnostic and Therapeutic Techniques, and Equipment [E], Psychiatry and Psychology [F],Phenomena and Processes [G], Disciplines and Occupations [H], Anthropology, Education, Sociology, and Social Phenomena [I], Technology, Industry, and Agriculture [J], Information Science [L], Named Groups [M], Health Care [N], Geographicals [Z]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ccfff0adb6281e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Modéliser le problème d’apprentissage supervisé sur ces données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177269c5050b041",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "On décide de travailler uniquement sur l'abstractText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f51b2f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:18:27.147735Z",
     "start_time": "2024-01-12T07:18:27.034791Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = data['abstractText']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f006cc7fc94de",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Traiter vos données textuelles en supprimant les bruits dans les textes et en les normalisant. \n",
    "Vous pouvez vous inspirer par exemple du code par ici1 (en l’améliorant s’il le faut) si vous utiliserez la librairie NLTK. Vous pouvez aussi utiliser d’autres librairies (A vos recherches). N’oubliez pas que cette étape de pré-traitement (preprocessing) dépend de vos données et du problème traité. Quelques traitements à faire sont :\n",
    "- Suppression des ponctuations comme . , ! $( ) * % @ o Suppression des URLs\n",
    "- Suppression des Stop words\n",
    "- Transformation de tout le texte en minuscule.\n",
    "- Tokenisation de vos textes\n",
    "- Racinisation (Stemming)\n",
    "- Lemmatisation (lemmatization) o Etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6392337609a66fc9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:18:45.525455Z",
     "start_time": "2024-01-12T07:18:43.018786Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample = remove_URL(sample)\n",
    "    sample = replace_contractions(sample)\n",
    "    # Tokenize\n",
    "    words = nltk.word_tokenize(sample)\n",
    "\n",
    "    # Normalize\n",
    "    return normalize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4801c525f38e31ee",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:18:46.466694Z",
     "start_time": "2024-01-12T07:18:46.308520Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/floriangeillon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c524d687307dec8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:22:00.584941Z",
     "start_time": "2024-01-10T13:20:49.910858Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:10<00:00, 70.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "words_data = []\n",
    "for sample in tqdm(corpus):\n",
    "    sample = remove_URL(sample)\n",
    "    sample = replace_contractions(sample)\n",
    "\n",
    "    # Tokenize\n",
    "    words = nltk.word_tokenize(sample)\n",
    "\n",
    "    # Normalize\n",
    "    words = normalize(words)\n",
    "\n",
    "    words_data.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b02cf99e2fcb1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:22:00.692307Z",
     "start_time": "2024-01-10T13:22:00.586146Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('words_data.pkl', 'wb') as f:\n",
    "    pickle.dump(words_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26e3d150c455766f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:18:50.963327Z",
     "start_time": "2024-01-12T07:18:50.900323Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('words_data.pkl', 'rb') as f:\n",
    "    words_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5789a3ae6414ed65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:22:00.761800Z",
     "start_time": "2024-01-10T13:22:00.758310Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['members', 'threezincfinger', 'family', 'transcription', 'factors', 'play', 'important', 'role', 'determining', 'basal', 'transcription', 'cloned', 'mouse', 'bteb3', 'mbteb3', 'new', 'member', 'basic', 'transcription', 'element', 'binding', 'protein', 'bteb', 'family', 'expressed', 'wide', 'variety', 'tissues', 'mbteb3', 'activates', 'transcription', 'simian', 'virus', 'forty', 'early', 'promoter', '4fold', 'tissuespecific', 'sm22alpha', 'promoter', '100fold', 'suggesting', 'like', 'bteb1', 'sp1', 'mbteb3', 'basal', 'transcription', 'factor']\n"
     ]
    }
   ],
   "source": [
    "print(words_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4639acb936e94",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Séparer les données en jeu de données d’apprentissage et jeu de données de test (50-50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e20c617906225",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:18:54.727720Z",
     "start_time": "2024-01-12T07:18:54.701261Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(words_data, data[\n",
    "    ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']], test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87787f464828ee57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Proposer une fonction run_models permettant de comparer plusieurs modèles d’apprentissage (en fonction de votre modélisation) sur ces données. \n",
    "Pour les approches multi-label, vous utiliserez l’approche EnsembleClassifierChain et MultiOutputCLassifier du package sklearn.multioutput. Ces classifieurs nécessite un classifieur de base (base_estimator) dont vous avez le libre choix d’utilisation. Votre évaluation se basera sur les deux mesures hamming_loss et zero_one_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e02fe9202ba49536",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:18:58.580734Z",
     "start_time": "2024-01-12T07:18:58.566394Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "\n",
    "def run_models(X_train, y_train, X_test, y_test, models, file_names=[]):\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"Hamming loss: \", hamming_loss(y_test, y_pred))\n",
    "        print(\"Zero-one loss: \", zero_one_loss(y_test, y_pred))\n",
    "        print()\n",
    "        if len(file_names) > 0:\n",
    "            with open(file_names[models.index(model)], 'wb') as f:\n",
    "                pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdc69d42c28fa9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "On choisit d'utiliser un classifieur de base RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "267ddf0e0e14944d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:22:04.515853Z",
     "start_time": "2024-01-12T07:22:04.417397Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "base_estimator = RandomForestClassifier()\n",
    "\n",
    "models = [ClassifierChain(base_estimator=base_estimator),\n",
    "          MultiOutputClassifier(estimator=base_estimator)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc38133ee3a3fcb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Proposer une première vectorisation de vos données textuelles par une représentation TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b55561899640b902",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:23:39.837052Z",
     "start_time": "2024-01-12T07:23:39.155296Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train_tfidf = [' '.join(words) for words in X_train]\n",
    "X_test_tfidf = [' '.join(words) for words in X_test]\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = vectorizer.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d74d0e82d6ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exécuter ensuite votre fonction run_models sur vos données et interpréter les résultats obtenus.\n",
    "Vous pouvez toujours appliquer votre fonction sur les données pré-traitées et les données non pré-\n",
    "traitées afin d’analyser l’apport de la partie pré-traitement de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe62435e411d693d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:22:46.993622Z",
     "start_time": "2024-01-10T13:22:04.415470Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain(base_estimator=RandomForestClassifier())\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mrun_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_tfidf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test_tfidf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m           \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodels/ClassifierChain_ap.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodels/MultiOutputClassifier_ap.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[9], line 10\u001B[0m, in \u001B[0;36mrun_models\u001B[1;34m(X_train, y_train, X_test, y_test, models, file_names)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m models:\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(model)\n\u001B[1;32m---> 10\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHamming loss: \u001B[39m\u001B[38;5;124m\"\u001B[39m, hamming_loss(y_test, y_pred))\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[0;32m   1150\u001B[0m ):\n\u001B[1;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\multioutput.py:928\u001B[0m, in \u001B[0;36mClassifierChain.fit\u001B[1;34m(self, X, Y, **fit_params)\u001B[0m\n\u001B[0;32m    922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fit_params \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _routing_enabled():\n\u001B[0;32m    923\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    924\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_params is only supported if enable_metadata_routing=True. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    925\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee the User Guide for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    926\u001B[0m     )\n\u001B[1;32m--> 928\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    930\u001B[0m     estimator\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;28;01mfor\u001B[39;00m chain_idx, estimator \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_)\n\u001B[0;32m    931\u001B[0m ]\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\multioutput.py:726\u001B[0m, in \u001B[0;36m_BaseChain.fit\u001B[1;34m(self, X, Y, **fit_params)\u001B[0m\n\u001B[0;32m    724\u001B[0m y \u001B[38;5;241m=\u001B[39m Y[:, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morder_[chain_idx]]\n\u001B[0;32m    725\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChain\u001B[39m\u001B[38;5;124m\"\u001B[39m, message):\n\u001B[1;32m--> 726\u001B[0m     \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    727\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX_aug\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mchain_idx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    728\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    729\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    730\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    732\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m chain_idx \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    733\u001B[0m     col_idx \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m chain_idx\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[0;32m   1150\u001B[0m ):\n\u001B[1;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    445\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    447\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    448\u001B[0m ]\n\u001B[0;32m    450\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 456\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    463\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    468\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     64\u001B[0m )\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\joblib\\parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\joblib\\parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    125\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    186\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 188\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    190\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[0;32m   1150\u001B[0m ):\n\u001B[1;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    928\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    929\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    930\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001B[39;00m\n\u001B[0;32m    931\u001B[0m \n\u001B[0;32m    932\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m    957\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 959\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    961\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    965\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32md:\\code\\Python\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[0;32m    432\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    433\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    434\u001B[0m         splitter,\n\u001B[0;32m    435\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    440\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    441\u001B[0m     )\n\u001B[1;32m--> 443\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "run_models(X_train_tfidf, y_train, X_test_tfidf, y_test, models,\n",
    "           ['models/ClassifierChain_ap.pkl', 'models/MultiOutputClassifier_ap.pkl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a83b7bba0db59",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sans prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca2912e71a5d6b0d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:24:11.407679Z",
     "start_time": "2024-01-12T07:24:10.547603Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corpus = data['abstractText']\n",
    "vectorizer_sp = TfidfVectorizer(stop_words=None)\n",
    "\n",
    "X_train_sp, X_test_sp, y_train_sp, y_test_sp = train_test_split(corpus, data[\n",
    "    ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']], test_size=0.5, random_state=0)\n",
    "\n",
    "X_train_tfidf_sp = vectorizer_sp.fit_transform(X_train_sp)\n",
    "X_test_tfidf_sp = vectorizer_sp.transform(X_test_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "277460de371a3697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:23:34.139102Z",
     "start_time": "2024-01-10T13:22:47.537610Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain(base_estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.16888571428571428\n",
      "Zero-one loss:  0.9088\n",
      "\n",
      "MultiOutputClassifier(estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.16937142857142856\n",
      "Zero-one loss:  0.9004\n"
     ]
    }
   ],
   "source": [
    "run_models(X_train_tfidf_sp, y_train_sp, X_test_tfidf_sp, y_test_sp, models,\n",
    "           ['models/ClassifierChain_sp.pkl', 'models/MultiOutputClassifier_sp.pkl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee7d936e82d8dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pas énormément de différence entre les deux approches (avec et sans prétraitement) peut être à cause de RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826bc334568267b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Appliquer la méthode SVD de réduction de dimensions (TruncatedSVD) afin de construire des \"concepts\" liés aux documents et aux termes. Elle permettra entre autres de résoudre les problèmes de synonymie (plusieurs mots avec un seul sens) et de polysémie (un seul mot avec plusieurs sens). \n",
    "La fonction suivante vous aidera à comprendre les concepts en affichant leurs mots les plus pertinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845ce596cc1fc992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:23:34.141473Z",
     "start_time": "2024-01-10T13:23:34.138059Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Concept #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca75a0cb6c138f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:23:35.357561Z",
     "start_time": "2024-01-10T13:23:34.140515Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TruncatedSVD(n_components=100, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=100, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TruncatedSVD(n_components=100, random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SVD = TruncatedSVD(n_components=100, random_state=0)\n",
    "SVD.fit(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b41266401bc6cf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:23:35.593844Z",
     "start_time": "2024-01-10T13:23:35.338134Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept #0: patients thousand ninety seven thirty group study treatment cells eighty\n",
      "Concept #1: cells cell expression protein gene activity dna human proteins binding\n",
      "Concept #2: thousand ninety ci health seven eighty seventy thirty million women\n",
      "Concept #3: thousand cells cancer ninety cell ci expression eighty seven tumor\n",
      "Concept #4: group rats glucose insulin groups plasma levels control blood significantly\n",
      "Concept #5: health cells cancer group cell care women levels risk effects\n",
      "Concept #6: gene expression genes group patients protein health proteins levels family\n",
      "Concept #7: group cancer groups breast method control lung survival surgery tumor\n",
      "Concept #8: cancer treatment breast risk glucose therapy insulin activity lung prostate\n",
      "Concept #9: expression gene genes left case right lung subjects pressure exercise\n",
      "Concept #10: infection strains isolates virus cases children species cells infections strain\n",
      "Concept #11: group case binding cases patient treatment report thousand disease children\n",
      "Concept #12: treatment therapy expression thousand rats days gene drug strains genes\n",
      "Concept #13: glucose insulin diabetes health liver acid content dna care acids\n",
      "Concept #14: glucose cells insulin dna type diabetes subjects treatment children memory\n",
      "Concept #15: insulin binding thousand receptor glucose infection mice levels serum antibodies\n",
      "Concept #16: glucose coronary insulin myocardial isolates drug strains diabetes cardiac resistance\n",
      "Concept #17: dna rna cancer rats women coronary care cells days sites\n",
      "Concept #18: treatment children cell therapy women binding blood receptor mean years\n",
      "Concept #19: ci ninety renal liver children model mortality transplantation method failure\n",
      "Concept #20: pain activity expression ph ninety protein proteins test ci cases\n",
      "Concept #21: acid ci ninety risk mortality lung ph pulmonary infection species\n",
      "Concept #22: species children women apoptosis mortality years cancer risk complications surgery\n",
      "Concept #23: children cell rats bone ninety health seven test thirty ci\n",
      "Concept #24: children lung cells pulmonary years cancer family ms ph normal\n",
      "Concept #25: children dna lung cell cancer transplantation years activation mice mean\n",
      "Concept #26: lung dna pulmonary thousand expression children injury exposure pain tb\n",
      "Concept #27: proteins rna lung women treatment protein synthesis therapy bone activity\n",
      "Concept #28: species disease cell dna acid ph response myocardial coronary treatment\n",
      "Concept #29: renal isolates treatment ph bone ninety ci strains degrees proteins\n",
      "Concept #30: cell ph infection growth binding drug water days rats rate\n",
      "Concept #31: pain care strains cell isolates apoptosis protein imaging subjects proteins\n",
      "Concept #32: species bone growth pain levels exercise months therapy binding renal\n",
      "Concept #33: bone drug acid cancer protein cases proteins liver serum myocardial\n",
      "Concept #34: disease bone mice pain lung diabetes model care women degrees\n",
      "Concept #35: exercise apoptosis cases women species model protein proteins thirty changes\n",
      "Concept #36: women liver platelet sleep pregnancy beta activation therapy rats exercise\n",
      "Concept #37: family pain women infection children structure platelet transplantation renal plasma\n",
      "Concept #38: lung pain tumor type platelet serum liver surgical tumors resection\n",
      "Concept #39: pain rats children pressure species proteins method disease blood activation\n",
      "Concept #40: sleep rats renal species health exercise months transplantation subjects thirty\n",
      "Concept #41: ph bone growth test cocaine acid blood pancreatic mm mice\n",
      "Concept #42: drug liver sleep imaging risk rna species expression disease mr\n",
      "Concept #43: renal coronary receptor artery vascular isolates food intake disease methadone\n",
      "Concept #44: cases health days subjects bone isolates months assay exposure binding\n",
      "Concept #45: platelet family injury blood activity species bone cases enzyme clinical\n",
      "Concept #46: therapy injury test dna degrees family pulmonary strains antibodies diabetes\n",
      "Concept #47: sleep diabetes mice type method growth strains responses family stroke\n",
      "Concept #48: pain ph treatment liver binding cases training children stress ca\n",
      "Concept #49: sleep drug model bone antibodies children plasma injury prevalence therapy\n",
      "Concept #50: care plasma degrees control temperature rats stress strains tumor pregnancy\n",
      "Concept #51: ph rna type diabetes mrna method receptor care hiv growth\n",
      "Concept #52: lead acid disease skin degrees rats plasma infection sensitivity symptoms\n",
      "Concept #53: isolates model exercise reaction time sleep acid tumor ca changes\n",
      "Concept #54: platelet water gene activation apoptosis virus antibodies tissue tb body\n",
      "Concept #55: lead women plasma health samples presented case control liver concentrations\n",
      "Concept #56: days disease cocaine lung tb rats trna rna test transplantation\n",
      "Concept #57: drug mm expression dna cocaine family clinical release renal days\n",
      "Concept #58: strains genes cases ca isolated trna months rna sensitivity change\n",
      "Concept #59: sleep pulmonary rna lead effects synthesis care vascular serum ci\n",
      "Concept #60: ca prevalence human dna lead renal genetic mass data mm\n",
      "Concept #61: years tb pulmonary mortality bone analysis insulin beta concentration research\n",
      "Concept #62: serum health exercise children test women body thousand model drug\n",
      "Concept #63: family neurons effect pulmonary genetic platelet children months method volume\n",
      "Concept #64: sleep lung stress changes apoptosis plasma membrane coronary care status\n",
      "Concept #65: water vitamin platelet activation vs rats coli samples iron clinical\n",
      "Concept #66: pulmonary treatment tb number surgery prostate drug age performance levels\n",
      "Concept #67: water ph technique lead rats size subjects model serum artery\n",
      "Concept #68: water injury quality mice vs sleep iron tumours rna receptors\n",
      "Concept #69: tb bone human cea plasma stroke artery bond seven intake\n",
      "Concept #70: isolates enzyme levels tumor ii information data compared cases smoking\n",
      "Concept #71: neurons glucose gene treatment release receptors salt lung children temperature\n",
      "Concept #72: transplantation sleep model gastric treatment pancreatic cocaine tumors content patient\n",
      "Concept #73: clinical type test strain children outcomes uptake insulin ed volume\n",
      "Concept #74: mortality blood technique type exercise diabetes pulmonary major water iron\n",
      "Concept #75: sequences tb sleep transplantation exposure lead analysis women rna dental\n",
      "Concept #76: cocaine trna plasma samples diabetes quality molecules exposure students transplantation\n",
      "Concept #77: lead human factors coli control sites antigen trna body anxiety\n",
      "Concept #78: pulmonary cocaine neurons levels uptake research myocardial quality use body\n",
      "Concept #79: ca cr compounds patient data prevalence concentration carcinoma temperature antibodies\n",
      "Concept #80: sleep method tumor phase size water subjects vitro smoking apoptosis\n",
      "Concept #81: transplantation thirty levels platelet mm proteins clinical rats model ca\n",
      "Concept #82: population exercise volume transplantation treatment anemia gastric lesions high strains\n",
      "Concept #83: molecules lead water dogs number movement increased years mirnas calcium\n",
      "Concept #84: cocaine ischemia genes human high therapy medical insulin brain microm\n",
      "Concept #85: ca development gastric cases risk assay transfer specificity rna cocaine\n",
      "Concept #86: tissue ca implant program ii risk lead resistance research fluid\n",
      "Concept #87: skin subjects ca health pancreatic therapy selection contact genes rate\n",
      "Concept #88: gastric surgery transplantation skin content compression apoptosis implant blood coli\n",
      "Concept #89: mass lead subjects response markers inflammatory complex sites ms gastric\n",
      "Concept #90: lead reaction genetic serum dose information synthesis strain concentrations injury\n",
      "Concept #91: responses response smoking bacterial ms iron day beta years sites\n",
      "Concept #92: high mirnas change surgery syndrome obesity subjects effects exercise soil\n",
      "Concept #93: cocaine spinal process complex reaction use transplantation tumours tumour synthesis\n",
      "Concept #94: dogs serum hypertension tumours cocaine girls survival il6 exposure embryos\n",
      "Concept #95: tb anxiety mirnas control technique studies monitoring rats mirna activation\n",
      "Concept #96: calcium exposure vs case angiotensin transfer antioxidant proteins volume symptoms\n",
      "Concept #97: beta sites dogs tr level prostate inflammatory crc flow behavior\n",
      "Concept #98: months rate mrna plasma infants molecules body rna gastric hiv\n",
      "Concept #99: diabetes effect change apoptosis blood nanoparticles prevalence robot development high\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print_top_words(SVD, feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67d29fc4ea7342",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Même chose avec les données sans prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d654e609500364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:23:36.901454Z",
     "start_time": "2024-01-10T13:23:35.593644Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept #0: the of and in to with was were for patients\n",
      "Concept #1: patients with were group was and had treatment who years\n",
      "Concept #2: cells cell expression in il induced levels protein cancer receptor\n",
      "Concept #3: group the was were mg rats kg groups glucose min\n",
      "Concept #4: and health to ci 95 care women risk for research\n",
      "Concept #5: il in group is rats glucose health insulin induced exercise\n",
      "Concept #6: cancer cells the cell ci 95 group breast tumor lung\n",
      "Concept #7: group in gene expression groups patients genes the were control\n",
      "Concept #8: group binding of patients the cancer glucose protein ci insulin\n",
      "Concept #9: il patients for alpha cancer treatment dose therapy binding group\n",
      "Concept #10: of health il cells hiv care infection were cell anti\n",
      "Concept #11: il the ifn expression 95 ci inflammatory mrna gene type\n",
      "Concept #12: and cancer case tumor glucose breast levels of acid treatment\n",
      "Concept #13: and health care protein binding cell patients beta cells the\n",
      "Concept #14: expression gene treatment genes of therapy glucose insulin after mg\n",
      "Concept #15: group patient case is old binding year beta after acid\n",
      "Concept #16: glucose insulin is diabetes group ml plasma dna method liver\n",
      "Concept #17: glucose cell dna insulin subjects treatment diabetes therapy with on\n",
      "Concept #18: of group cells sleep platelet subjects women coronary groups pressure\n",
      "Concept #19: acid apoptosis ci 95 caspase activity induced mortality glucose group\n",
      "Concept #20: hiv drug coronary myocardial the resistance isolates cardiac infection strains\n",
      "Concept #21: children renal hiv treatment transplantation liver and mg ci the\n",
      "Concept #22: insulin glucose binding of receptor diabetes cells were treatment cases\n",
      "Concept #23: dna and rats rna liver on sites species cancer of\n",
      "Concept #24: was activity dna apoptosis beta pain in induced method for\n",
      "Concept #25: hiv infection proteins protein virus acid to liver surgery after\n",
      "Concept #26: hiv cancer apoptosis ml years dna mean caspase and beta\n",
      "Concept #27: exercise ml hiv with sleep min cancer ci 95 during\n",
      "Concept #28: dna is pain water that by atp cell ph mg\n",
      "Concept #29: beta acid health with apoptosis from we model alpha species\n",
      "Concept #30: cell model was drug of species in coronary children anti\n",
      "Concept #31: hiv rna glucose drug species expression lung treatment cells pain\n",
      "Concept #32: hiv bone health gene as beta or be acid exercise\n",
      "Concept #33: pain cells drug acid body mg isolates infection this resistance\n",
      "Concept #34: exercise acid were sleep as cases during blood therapy cancer\n",
      "Concept #35: lung pulmonary activity children volume pressure by dose hypertension rna\n",
      "Concept #36: and mg kg lung body disease weight for virus anti\n",
      "Concept #37: protein exercise children proteins mass body treatment growth species that\n",
      "Concept #38: cell acid subjects for of data liver volume cocaine glucose\n",
      "Concept #39: exercise rats model bone high growth activity care for drug\n",
      "Concept #40: bone treatment women with cells disease proteins pain protein igf\n",
      "Concept #41: liver cells as hiv to children disease ph acid degrees\n",
      "Concept #42: ph cocaine pressure hypertension blood from treatment years drug samples\n",
      "Concept #43: renal on growth cancer activity insulin prostate model body care\n",
      "Concept #44: lung cells was pulmonary model species renal with insulin their\n",
      "Concept #45: pain sleep platelet species care months antibodies tissue are health\n",
      "Concept #46: mice cocaine renal health diabetes binding with species as strain\n",
      "Concept #47: levels diabetes cases type were after than cell liver bond\n",
      "Concept #48: rats ml cell plasma children activity vitamin is that women\n",
      "Concept #49: pain bone pulmonary mg growth diabetes apoptosis health ci hypertension\n",
      "Concept #50: renal to alpha ii mass exercise pain hiv risk tb\n",
      "Concept #51: beta pain tumor alpha tumors plasma self mice pulmonary change\n",
      "Concept #52: as liver platelet activity sleep mass were well infection species\n",
      "Concept #53: sleep been has hiv cell have care drug intake food\n",
      "Concept #54: liver health lung transplantation coronary bone been test after exercise\n",
      "Concept #55: species apoptosis glucose is malaria ml caspase between there pain\n",
      "Concept #56: disease lead tb cases health ml case infection symptoms be\n",
      "Concept #57: sleep infection pain virus subjects acid growth cd and cases\n",
      "Concept #58: as cocaine isolates disease synthesis water beta bone training exercise\n",
      "Concept #59: sleep were or rats lead model coronary il apoptosis obesity\n",
      "Concept #60: dose mm rna pain risk ct disease children imaging are\n",
      "Concept #61: or water care ph sleep liver body anti pregnancy dentin\n",
      "Concept #62: sensitivity insulin pulmonary liver response mice transplantation receptor ar lead\n",
      "Concept #63: ph hiv after rats calcium risk proteins method children cases\n",
      "Concept #64: family sleep than cocaine injury more species transplantation risk 01\n",
      "Concept #65: cocaine virus pulmonary had phase strains ph beta subjects drug\n",
      "Concept #66: to bone injury family serum ph with model sensitivity days\n",
      "Concept #67: expression species mrna liver beta is mice on pet up\n",
      "Concept #68: body to program pain lead method rna women levels with\n",
      "Concept #69: on pain prevalence enzyme mice ar pulmonary lead schizophrenia sleep\n",
      "Concept #70: by women platelet anxiety lung binding ii cocaine water liver\n",
      "Concept #71: family hiv degrees method anti igf tumor visual subjects temperature\n",
      "Concept #72: disease apoptosis insulin dna mortality water artery high cea caspase\n",
      "Concept #73: phase type patient diabetes is during pain cocaine anti trna\n",
      "Concept #74: cocaine tb is salt changes training drug pancreatic symptoms with\n",
      "Concept #75: alpha pet synthesis three side rna acute by family aspirin\n",
      "Concept #76: diabetes bone tb plasma ml injury on hr type prostate\n",
      "Concept #77: ca patient infection type was are both per more diabetes\n",
      "Concept #78: injury cocaine volume severe prostate pain insulin are therapy tumors\n",
      "Concept #79: sleep cocaine bone ii dose lung use species response myocardial\n",
      "Concept #80: model cocaine mass genes or gene risk mice lung been\n",
      "Concept #81: calcium hla training ph muscle based lead two model risk\n",
      "Concept #82: growth liver 10 case strains human their treatment girls peptide\n",
      "Concept #83: water children after women drug mm hla disease type receptor\n",
      "Concept #84: igf ca mm clinical therapy may lung be oh receptor\n",
      "Concept #85: ca reaction care tissue 000 pulmonary risk tumor therapy proteins\n",
      "Concept #86: cocaine are human breast mir insulin training activity was or\n",
      "Concept #87: pulmonary platelet family strains cell diabetes or their control case\n",
      "Concept #88: liver isolates malaria pulmonary system mass body is lead gastric\n",
      "Concept #89: subjects treatment tissue hpv mortality using disease high more receptor\n",
      "Concept #90: mass ms response transplantation prevalence hpv smoking cocaine trna is\n",
      "Concept #91: ii water use from pd after mass for ca activation\n",
      "Concept #92: mir isolates data clinical expression virus mirnas membrane anti presented\n",
      "Concept #93: hpv method cocaine therapy patient muscle vs isolates number test\n",
      "Concept #94: rats tumor control effect pancreatic body had pd clinical level\n",
      "Concept #95: hpv alpha is family mental not factors 01 pulmonary infants\n",
      "Concept #96: mir igf 10 mirnas igfbp students exposure injury family diabetes\n",
      "Concept #97: degrees their cases transplantation students pulmonary an syndrome strains synthesis\n",
      "Concept #98: tb transplantation 01 by genes quality rats system ca exercise\n",
      "Concept #99: during mm alpha development hla ms body growth their aa\n"
     ]
    }
   ],
   "source": [
    "SVD_sp = TruncatedSVD(n_components=100, random_state=0)\n",
    "SVD_sp.fit(X_train_tfidf_sp)\n",
    "\n",
    "feature_names_sp = vectorizer_sp.get_feature_names_out()\n",
    "print_top_words(SVD_sp, feature_names_sp, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d0e783c74c3bb9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:23:36.970562Z",
     "start_time": "2024-01-10T13:23:36.900534Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tfidf_svd = SVD.transform(X_train_tfidf)\n",
    "X_test_tfidf_svd = SVD.transform(X_test_tfidf)\n",
    "\n",
    "X_train_tfidf_sp_svd = SVD_sp.transform(X_train_tfidf_sp)\n",
    "X_test_tfidf_sp_svd = SVD_sp.transform(X_test_tfidf_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49e70a9c53ea7c78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:24:36.263326Z",
     "start_time": "2024-01-10T13:23:36.964826Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain(base_estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.16357142857142856\n",
      "Zero-one loss:  0.8972\n",
      "\n",
      "MultiOutputClassifier(estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.1652\n",
      "Zero-one loss:  0.9052\n"
     ]
    }
   ],
   "source": [
    "run_models(X_train_tfidf_svd, y_train, X_test_tfidf_svd, y_test, models,\n",
    "           ['models/ClassifierChain_ap_svd.pkl', 'models/MultiOutputClassifier_ap_svd.pkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a102effaf8019fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:25:37.308906Z",
     "start_time": "2024-01-10T13:24:36.264867Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain(base_estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.16785714285714284\n",
      "Zero-one loss:  0.9092\n",
      "\n",
      "MultiOutputClassifier(estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.16994285714285715\n",
      "Zero-one loss:  0.918\n"
     ]
    }
   ],
   "source": [
    "run_models(X_train_tfidf_sp_svd, y_train_sp, X_test_tfidf_sp_svd, y_test_sp, models,\n",
    "           ['models/ClassifierChain_sp_svd.pkl', 'models/MultiOutputClassifier_sp_svd.pkl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633fd21b52509be0",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2ae72dc",
   "metadata": {},
   "source": [
    "### Proposer uncode qui permettra d’apprendre votre propre modèle de plongement lexical Word2Vec sur vos données textuelles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11be05b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:19:19.208400Z",
     "start_time": "2024-01-12T07:19:15.665028Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(words_data, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb82ef2",
   "metadata": {},
   "source": [
    "### Évaluer visuellement et numériquement sur quelques mots clés votre nouveaumodèle de vectorisation (Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "984c0a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:19:25.676624Z",
     "start_time": "2024-01-12T07:19:25.553100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('breast', 0.9533255696296692),\n ('receptorpositive', 0.921899676322937),\n ('leukaemia', 0.9211985468864441),\n ('earlystage', 0.920826256275177),\n ('metastatic', 0.9075683355331421),\n ('prostate', 0.9035713076591492),\n ('nonmalignant', 0.9009960293769836),\n ('lung', 0.8916445374488831),\n ('colorectal', 0.8911237716674805),\n ('progression', 0.8892887234687805)]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ee84ac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:19:27.105612Z",
     "start_time": "2024-01-12T07:19:27.095801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'bike'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['diabetes', 'heart', 'bike'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e586fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:21:39.737128Z",
     "start_time": "2024-01-12T07:19:38.353157Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m tsne \u001B[38;5;241m=\u001B[39m TSNE(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, n_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m250\u001B[39m, perplexity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      8\u001B[0m np\u001B[38;5;241m.\u001B[39mset_printoptions(suppress\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 9\u001B[0m T \u001B[38;5;241m=\u001B[39m \u001B[43mtsne\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwvs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m labels \u001B[38;5;241m=\u001B[39m words\n\u001B[1;32m     12\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m14\u001B[39m, \u001B[38;5;241m8\u001B[39m))\n",
      "File \u001B[0;32m~/Documents/Cours/Master/S3/TAA/TAA-TP3/pythonProject/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    163\u001B[0m         )\n",
      "File \u001B[0;32m~/Documents/Cours/Master/S3/TAA/TAA-TP3/pythonProject/.venv/lib/python3.11/site-packages/sklearn/base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1150\u001B[0m     )\n\u001B[1;32m   1151\u001B[0m ):\n\u001B[0;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Cours/Master/S3/TAA/TAA-TP3/pythonProject/.venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1111\u001B[0m, in \u001B[0;36mTSNE.fit_transform\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m   1090\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001B[39;00m\n\u001B[1;32m   1091\u001B[0m \n\u001B[1;32m   1092\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001B[39;00m\n\u001B[1;32m   1109\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1110\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params_vs_input(X)\n\u001B[0;32m-> 1111\u001B[0m embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1112\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_ \u001B[38;5;241m=\u001B[39m embedding\n\u001B[1;32m   1113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_\n",
      "File \u001B[0;32m~/Documents/Cours/Master/S3/TAA/TAA-TP3/pythonProject/.venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1001\u001B[0m, in \u001B[0;36mTSNE._fit\u001B[0;34m(self, X, skip_num_points)\u001B[0m\n\u001B[1;32m    995\u001B[0m \u001B[38;5;66;03m# Degrees of freedom of the Student's t-distribution. The suggestion\u001B[39;00m\n\u001B[1;32m    996\u001B[0m \u001B[38;5;66;03m# degrees_of_freedom = n_components - 1 comes from\u001B[39;00m\n\u001B[1;32m    997\u001B[0m \u001B[38;5;66;03m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001B[39;00m\n\u001B[1;32m    998\u001B[0m \u001B[38;5;66;03m# Laurens van der Maaten, 2009.\u001B[39;00m\n\u001B[1;32m    999\u001B[0m degrees_of_freedom \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_components \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m-> 1001\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tsne\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1002\u001B[0m \u001B[43m    \u001B[49m\u001B[43mP\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1003\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdegrees_of_freedom\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1004\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_embedded\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_embedded\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1006\u001B[0m \u001B[43m    \u001B[49m\u001B[43mneighbors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneighbors_nn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_num_points\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_num_points\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1008\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Cours/Master/S3/TAA/TAA-TP3/pythonProject/.venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1053\u001B[0m, in \u001B[0;36mTSNE._tsne\u001B[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001B[0m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;66;03m# Learning schedule (part 1): do 250 iteration with lower momentum but\u001B[39;00m\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;66;03m# higher learning rate controlled via the early exaggeration parameter\u001B[39;00m\n\u001B[1;32m   1052\u001B[0m P \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mearly_exaggeration\n\u001B[0;32m-> 1053\u001B[0m params, kl_divergence, it \u001B[38;5;241m=\u001B[39m \u001B[43m_gradient_descent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mopt_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n\u001B[1;32m   1055\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m   1056\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[t-SNE] KL divergence after \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m iterations with early exaggeration: \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1057\u001B[0m         \u001B[38;5;241m%\u001B[39m (it \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, kl_divergence)\n\u001B[1;32m   1058\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/Cours/Master/S3/TAA/TAA-TP3/pythonProject/.venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:402\u001B[0m, in \u001B[0;36m_gradient_descent\u001B[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;66;03m# only compute the error when needed\u001B[39;00m\n\u001B[1;32m    400\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompute_error\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m check_convergence \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m n_iter \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 402\u001B[0m error, grad \u001B[38;5;241m=\u001B[39m \u001B[43mobjective\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    404\u001B[0m inc \u001B[38;5;241m=\u001B[39m update \u001B[38;5;241m*\u001B[39m grad \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m    405\u001B[0m dec \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39minvert(inc)\n",
      "File \u001B[0;32m~/Documents/Cours/Master/S3/TAA/TAA-TP3/pythonProject/.venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:283\u001B[0m, in \u001B[0;36m_kl_divergence_bh\u001B[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001B[0m\n\u001B[1;32m    280\u001B[0m indptr \u001B[38;5;241m=\u001B[39m P\u001B[38;5;241m.\u001B[39mindptr\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mint64, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    282\u001B[0m grad \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(X_embedded\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m--> 283\u001B[0m error \u001B[38;5;241m=\u001B[39m \u001B[43m_barnes_hut_tsne\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_P\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_embedded\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mneighbors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindptr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m    \u001B[49m\u001B[43mangle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_components\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdof\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdegrees_of_freedom\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_error\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    296\u001B[0m c \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m*\u001B[39m (degrees_of_freedom \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1.0\u001B[39m) \u001B[38;5;241m/\u001B[39m degrees_of_freedom\n\u001B[1;32m    297\u001B[0m grad \u001B[38;5;241m=\u001B[39m grad\u001B[38;5;241m.\u001B[39mravel()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# evaluer visuellement (graphique)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = list(model.wv.index_to_key)\n",
    "wvs = model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=250, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x + 1, y + 1), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc83b0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:21:42.803662Z",
     "start_time": "2024-01-12T07:21:42.791141Z"
    }
   },
   "outputs": [],
   "source": [
    "def word2vec_generator(texts, model, vector_size):\n",
    "    dict_word2vec = {}\n",
    "    for index, word_list in enumerate(texts):\n",
    "        arr = np.array([0.0 for i in range(0, vector_size)])\n",
    "        nb_word = 0\n",
    "        for word in word_list:\n",
    "            try:\n",
    "                arr += model.wv[word]\n",
    "                nb_word = nb_word + 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if (len(word_list) == 0):\n",
    "            dict_word2vec[index] = arr\n",
    "        else:\n",
    "            dict_word2vec[index] = arr / nb_word\n",
    "    df_word2vec = pd.DataFrame(dict_word2vec).T\n",
    "    return df_word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d4efdb",
   "metadata": {},
   "source": [
    "### Exploiter votre modèle Word2Vec pour la vectorisation de vos textes (avec deux méthodes utilisant ou non le TF-IDF des mots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a457fccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:23:05.763181Z",
     "start_time": "2024-01-12T07:22:11.989409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain(base_estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.18177142857142858\n",
      "Zero-one loss:  0.9268\n",
      "\n",
      "MultiOutputClassifier(estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.1818\n",
      "Zero-one loss:  0.9204\n"
     ]
    }
   ],
   "source": [
    "# 1. avec prétraitement\n",
    "X_train_w2v = word2vec_generator(X_train, model, 100)\n",
    "X_test_w2v = word2vec_generator(X_test, model, 100)\n",
    "\n",
    "run_models(X_train_w2v, y_train, X_test_w2v, y_test, models,\n",
    "           ['models/ClassifierChain_ap_w2v.pkl', 'models/MultiOutputClassifier_ap_w2v.pkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d80529bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:26:29.715500Z",
     "start_time": "2024-01-12T07:25:20.291953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain(base_estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.2610571428571429\n",
      "Zero-one loss:  0.9772\n",
      "\n",
      "MultiOutputClassifier(estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.25948571428571426\n",
      "Zero-one loss:  0.9808\n"
     ]
    }
   ],
   "source": [
    "# 2. sans prétraitement\n",
    "X_train_w2v_sp = word2vec_generator(X_train_sp, model, 100)\n",
    "X_test_w2v_sp = word2vec_generator(X_test_sp, model, 100)\n",
    "\n",
    "run_models(X_train_w2v_sp, y_train_sp, X_test_w2v_sp, y_test_sp, models,\n",
    "           ['models/ClassifierChain_sp_w2v.pkl', 'models/MultiOutputClassifier_sp_w2v.pkl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d2064",
   "metadata": {},
   "source": [
    "### Idem en utilisant le modèle Word2Vec pré-entrainé de Google (GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51cf1906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:31:32.294296Z",
     "start_time": "2024-01-12T07:31:32.282872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variation du code car on à un KeyedVectors et non un Word2Vec\n",
    "def word2vec_generator_google(texts, model, vector_size):\n",
    "    dict_word2vec = {}\n",
    "    for index, word_list in enumerate(texts):\n",
    "        arr = np.array([0.0 for i in range(0, vector_size)])\n",
    "        nb_word = 0\n",
    "        for word in word_list:\n",
    "            try:\n",
    "                arr += model[word]\n",
    "                nb_word = nb_word + 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if (len(word_list) == 0):\n",
    "            dict_word2vec[index] = arr\n",
    "        else:\n",
    "            dict_word2vec[index] = arr / nb_word\n",
    "    df_word2vec = pd.DataFrame(dict_word2vec).T\n",
    "    return df_word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "529078f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:33:37.692351Z",
     "start_time": "2024-01-12T07:33:22.361863Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_google = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e898cba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:35:33.105914Z",
     "start_time": "2024-01-12T07:33:47.533629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain(base_estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.15057142857142858\n",
      "Zero-one loss:  0.8835999999999999\n",
      "\n",
      "MultiOutputClassifier(estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.15228571428571427\n",
      "Zero-one loss:  0.8924\n"
     ]
    }
   ],
   "source": [
    "# 1. avec prétraitement\n",
    "X_train_w2v_google = word2vec_generator_google(X_train, model_google, 300)\n",
    "X_test_w2v_google = word2vec_generator_google(X_test, model_google, 300)\n",
    "\n",
    "run_models(X_train_w2v_google, y_train, X_test_w2v_google, y_test, models,\n",
    "           ['models/ClassifierChain_ap_w2v_google.pkl', 'models/MultiOutputClassifier_ap_w2v_google.pkl'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65a125ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:37:52.857243Z",
     "start_time": "2024-01-12T07:35:58.105604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain(base_estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.23108571428571428\n",
      "Zero-one loss:  0.9568\n",
      "\n",
      "MultiOutputClassifier(estimator=RandomForestClassifier())\n",
      "Hamming loss:  0.23174285714285714\n",
      "Zero-one loss:  0.9636\n"
     ]
    }
   ],
   "source": [
    "# 2. sans prétraitement\n",
    "X_train_w2v_google_sp = word2vec_generator_google(X_train_sp, model_google, 300)\n",
    "X_test_w2v_google_sp = word2vec_generator_google(X_test_sp, model_google, 300)\n",
    "\n",
    "run_models(X_train_w2v_google_sp, y_train_sp, X_test_w2v_google_sp, y_test_sp, models,\n",
    "           ['models/ClassifierChain_sp_w2v_google.pkl', 'models/MultiOutputClassifier_sp_w2v_google.pkl'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0fce00",
   "metadata": {},
   "source": [
    "### Exploiter votre modèle Word2Vec en entrée de la couche d’embedding d’un modèle à base de réseaux de neurones récurrents de type LSTM permettant :\n",
    "- Dans un premier temps d’optimiser la 0/1 loss.\n",
    "- Dans un deuxième temps d’optimiser la hamming loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1c0c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T17:53:27.002829Z",
     "start_time": "2024-01-12T17:53:21.905201Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b166e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T17:54:10.641453Z",
     "start_time": "2024-01-12T17:53:27.732351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 300)         900000300 \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 300)               721200    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 900721801 (3.36 GB)\n",
      "Trainable params: 721501 (2.75 MB)\n",
      "Non-trainable params: 900000300 (3.35 GB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = X_train\n",
    "test_tokenized = X_test\n",
    "\n",
    "\n",
    "def to_sequence(index, text):\n",
    "    indexes = [index[word] for word in text if word in index]\n",
    "    return indexes\n",
    "\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(model_google.index_to_key)}\n",
    "X_train_sequences = [to_sequence(word2idx, x) for x in train_tokenized]\n",
    "X_test_sequences = [to_sequence(word2idx, x) for x in test_tokenized]\n",
    "\n",
    "MAX_SEQ_LENGHT = 50\n",
    "N_FEATURES = len(model_google.index_to_key)\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
    "\n",
    "EMBEDDINGS_LEN = model_google.vector_size\n",
    "embeddings_index = np.zeros((len(model_google.index_to_key) + 1, EMBEDDINGS_LEN))\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    try:\n",
    "        embedding = model_google[word]\n",
    "        embeddings_index[idx] = embedding\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(len(model_google.key_to_index) + 1,\n",
    "                         EMBEDDINGS_LEN,\n",
    "                         weights=[embeddings_index],\n",
    "                         trainable=False))\n",
    "\n",
    "model_lstm.add(LSTM(300, dropout=0.2))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "225/225 [==============================] - 21s 84ms/step - loss: 0.6964 - val_loss: 0.6795\n",
      "Epoch 2/20\n",
      "225/225 [==============================] - 21s 93ms/step - loss: 0.6729 - val_loss: 0.6815\n",
      "Epoch 3/20\n",
      "225/225 [==============================] - 22s 96ms/step - loss: 0.6699 - val_loss: 0.6774\n",
      "Epoch 4/20\n",
      "225/225 [==============================] - 21s 92ms/step - loss: 0.6672 - val_loss: 0.6781\n",
      "Epoch 5/20\n",
      "225/225 [==============================] - 21s 93ms/step - loss: 0.6661 - val_loss: 0.6790\n",
      "Epoch 6/20\n",
      "225/225 [==============================] - 21s 94ms/step - loss: 0.6640 - val_loss: 0.6774\n",
      "Epoch 7/20\n",
      "225/225 [==============================] - 22s 96ms/step - loss: 0.6614 - val_loss: 0.6765\n",
      "Epoch 8/20\n",
      "225/225 [==============================] - 21s 95ms/step - loss: 0.6609 - val_loss: 0.6785\n",
      "Epoch 9/20\n",
      "225/225 [==============================] - 21s 95ms/step - loss: 0.6587 - val_loss: 0.6764\n",
      "Epoch 10/20\n",
      "225/225 [==============================] - 22s 99ms/step - loss: 0.6566 - val_loss: 0.6773\n",
      "Epoch 11/20\n",
      "225/225 [==============================] - 20s 91ms/step - loss: 0.6555 - val_loss: 0.6790\n",
      "Epoch 12/20\n",
      "225/225 [==============================] - 20s 89ms/step - loss: 0.6534 - val_loss: 0.6790\n",
      "Epoch 13/20\n",
      "225/225 [==============================] - 20s 88ms/step - loss: 0.6526 - val_loss: 0.6795\n",
      "Epoch 14/20\n",
      "225/225 [==============================] - 20s 91ms/step - loss: 0.6524 - val_loss: 0.6797\n",
      "Epoch 15/20\n",
      "225/225 [==============================] - 21s 91ms/step - loss: 0.6516 - val_loss: 0.6792\n",
      "Epoch 16/20\n",
      "225/225 [==============================] - 20s 89ms/step - loss: 0.6510 - val_loss: 0.6768\n",
      "Epoch 17/20\n",
      "225/225 [==============================] - 20s 89ms/step - loss: 0.6500 - val_loss: 0.6777\n",
      "Epoch 18/20\n",
      "225/225 [==============================] - 20s 91ms/step - loss: 0.6497 - val_loss: 0.6786\n",
      "Epoch 19/20\n",
      "225/225 [==============================] - 20s 90ms/step - loss: 0.6499 - val_loss: 0.6783\n",
      "Epoch 20/20\n",
      "225/225 [==============================] - 20s 90ms/step - loss: 0.6497 - val_loss: 0.6780\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x375167090>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model_lstm.fit(X_train_sequences, y_train, epochs=20, batch_size=10, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T18:01:05.962439Z",
     "start_time": "2024-01-12T17:54:10.646657Z"
    }
   },
   "id": "c35c5e8a4733a3a4",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "225/225 [==============================] - 20s 74ms/step - loss: 5.9768 - val_loss: 5.8697\n",
      "Epoch 2/20\n",
      "225/225 [==============================] - 20s 88ms/step - loss: 5.8153 - val_loss: 5.7956\n",
      "Epoch 3/20\n",
      "225/225 [==============================] - 18s 80ms/step - loss: 5.7668 - val_loss: 5.7791\n",
      "Epoch 4/20\n",
      "225/225 [==============================] - 18s 79ms/step - loss: 5.7461 - val_loss: 5.7667\n",
      "Epoch 5/20\n",
      "225/225 [==============================] - 19s 83ms/step - loss: 5.7343 - val_loss: 5.7548\n",
      "Epoch 6/20\n",
      "225/225 [==============================] - 18s 81ms/step - loss: 5.7277 - val_loss: 5.7502\n",
      "Epoch 7/20\n",
      "225/225 [==============================] - 18s 78ms/step - loss: 5.7244 - val_loss: 5.7505\n",
      "Epoch 8/20\n",
      "225/225 [==============================] - 18s 80ms/step - loss: 5.7261 - val_loss: 5.7518\n",
      "Epoch 9/20\n",
      "225/225 [==============================] - 18s 79ms/step - loss: 5.7258 - val_loss: 5.7485\n",
      "Epoch 10/20\n",
      "225/225 [==============================] - 18s 80ms/step - loss: 5.7214 - val_loss: 5.7446\n",
      "Epoch 11/20\n",
      "225/225 [==============================] - 19s 82ms/step - loss: 5.7218 - val_loss: 5.7480\n",
      "Epoch 12/20\n",
      "225/225 [==============================] - 35s 155ms/step - loss: 5.7207 - val_loss: 5.7436\n",
      "Epoch 13/20\n",
      "225/225 [==============================] - 17s 77ms/step - loss: 5.7204 - val_loss: 5.7446\n",
      "Epoch 14/20\n",
      "225/225 [==============================] - 17s 78ms/step - loss: 5.7213 - val_loss: 5.7457\n",
      "Epoch 15/20\n",
      "225/225 [==============================] - 18s 78ms/step - loss: 5.7195 - val_loss: 5.7437\n",
      "Epoch 16/20\n",
      "225/225 [==============================] - 17s 77ms/step - loss: 5.7195 - val_loss: 5.7472\n",
      "Epoch 17/20\n",
      "225/225 [==============================] - 18s 78ms/step - loss: 5.7200 - val_loss: 5.7453\n",
      "Epoch 18/20\n",
      "225/225 [==============================] - 18s 79ms/step - loss: 5.7196 - val_loss: 5.7456\n",
      "Epoch 19/20\n",
      "225/225 [==============================] - 18s 79ms/step - loss: 5.7188 - val_loss: 5.7434\n",
      "Epoch 20/20\n",
      "225/225 [==============================] - 17s 76ms/step - loss: 5.7194 - val_loss: 5.7456\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.utils import custom_object_scope\n",
    "\n",
    "\n",
    "def hamming_loss_handmade(y_true, y_pred):\n",
    "    return K.sum(K.abs(K.cast(y_true, 'float32') - y_pred), axis=-1)\n",
    "\n",
    "\n",
    "with custom_object_scope({'hamming_loss': hamming_loss_handmade}):\n",
    "    model_lstm.compile(loss='hamming_loss', optimizer='adam')\n",
    "    model_lstm.fit(X_train_sequences, y_train, epochs=20, batch_size=10, validation_split=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T07:50:34.992752Z",
     "start_time": "2024-01-12T07:44:17.530405Z"
    }
   },
   "id": "9539849fb80a0960",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline : Automatiser l’enchainement de votre meilleur traitement dans une fonction ou un pipeline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4004709fb1855b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=10000, suppress=True)\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Variation du code car on à un KeyedVectors et non un Word2Vec\n",
    "def word2vec_generator_google(texts, model, vector_size):\n",
    "    dict_word2vec = {}\n",
    "    for index, word_list in enumerate(texts):\n",
    "        arr = np.array([0.0 for i in range(0, vector_size)])\n",
    "        nb_word = 0\n",
    "        for word in word_list:\n",
    "            try:\n",
    "                arr += model[word]\n",
    "                nb_word = nb_word + 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if (len(word_list) == 0):\n",
    "            dict_word2vec[index] = arr\n",
    "        else:\n",
    "            dict_word2vec[index] = arr / nb_word\n",
    "    df_word2vec = pd.DataFrame(dict_word2vec).T\n",
    "    return df_word2vec\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_google = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('word2vec', FunctionTransformer(word2vec_generator_google, kw_args={'model': model_google, 'vector_size': 300})),\n",
    "    ('classifier', ClassifierChain(base_estimator=RandomForestClassifier()))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:46:45.031794Z",
     "start_time": "2024-01-12T17:46:28.956263Z"
    }
   },
   "id": "a51ec66f8260a384",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss:  0.15068571428571428\n",
      "Zero-one loss:  0.8855999999999999\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('words_data.pkl', 'rb') as f:\n",
    "    words_data = pickle.load(f)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('PubMed-multi-label-dataset.csv', sep=',', encoding=\"utf-8\")\n",
    "data = data.sample(frac=0.1, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(words_data, data[\n",
    "    ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']], test_size=0.5, random_state=0)\n",
    "\n",
    "from sklearn.metrics import hamming_loss, zero_one_loss\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Hamming loss: \", hamming_loss(y_test, y_pred))\n",
    "print(\"Zero-one loss: \", zero_one_loss(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:47:39.557964Z",
     "start_time": "2024-01-12T17:46:45.033669Z"
    }
   },
   "id": "dcfe8ddd120cc45b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e013c546f6d03826"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
